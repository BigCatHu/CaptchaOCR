{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、导入包以及设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# root = os.getcwd()               #获得当前路径 /home/dir1\n",
    "# print (root)\n",
    "# !cd r \"D:\\Learning_materials\\CS\\zero_one\\Jacob's_ladder\\Research_teaching_team\\Code\\ocr_start\\captcha_ocr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision  import transforms\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import random\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 二、以类的方式定义超参数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argparse():\n",
    "    def __init__(self) -> None:\n",
    "        self.captcha_size = 4\n",
    "        self.captcha_array = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.lr = 0.001\n",
    "        self.epochs = 20\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.data_train = np.array([-2, -1, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18, 20])\n",
    "        # self.data_val = np.array([15, 16, 17, 0.1, -3, -4])\n",
    "\n",
    "args = argparse()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 三、定义自己的模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mymodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mymodel,self).__init__()\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),  \n",
    "            nn.MaxPool2d(kernel_size=2), #[6, 64, 30, 80],\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), #[6, 128, 15, 40]\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # [6, 256, 7, 20]\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten()\n",
    "\n",
    "        )\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "          #[6, 2560] [64, 15360]\n",
    "          nn.Linear(in_features=15360,out_features=4096),\n",
    "          nn.Dropout(0.2),  # drop 20% of the neuron\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(in_features=4096, out_features = args.captcha_size*args.captcha_array.__len__())\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        x = self.layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myResNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(myResNet,self).__init__()\n",
    "        self.model = models.resnet50(pretrained=False)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.model.fc = nn.Linear(in_features=2048,out_features=args.captcha_size*args.captcha_array.__len__(), bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 五、定义自己的数据集Dataset,DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aab1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tools():\n",
    "\n",
    "    # def __init__(self):\n",
    "    #   pass\n",
    "\n",
    "    def text2vec(self, text):\n",
    "        # 4行36列\n",
    "        vec = torch.zeros((args.captcha_size, len(args.captcha_array)))\n",
    "        for i in range(len(text)):\n",
    "            vec[i, args.captcha_array.index(text[i])] = 1\n",
    "        return vec\n",
    "    # text2vec('aab1')\n",
    "\n",
    "    def vec2text(self, vec):\n",
    "        vec = torch.argmax(vec, dim = 1)\n",
    "        # print(vec)\n",
    "        text = ''\n",
    "        for v in vec:\n",
    "            text += args.captcha_array[v]\n",
    "        return text\n",
    "    \n",
    "    def accuracy(self, y_hat, y):  #@save\n",
    "        \"\"\"计算预测正确的数量\"\"\"\n",
    "            \n",
    "        if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "            y_hat = y_hat.argmax(axis=1)\n",
    "        cmp = y_hat.type(y.dtype) == y\n",
    "        return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "tls = Tools()\n",
    "tls.vec2text(tls.text2vec('aab1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(args.captcha_array) == args.captcha_array.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class My_datasets(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        super(My_datasets, self).__init__()\n",
    "        # self.image_path = os.listdir(root_dir)\n",
    "        self.list_image_path = [os.path.join(root_dir, image_path) for image_path in os.listdir(root_dir)]\n",
    "\n",
    "        self.transforms = transforms.Compose(\n",
    "            [\n",
    "            transforms.Resize((60,160)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # print(self.list_image_path)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        image_path = self.list_image_path[index]\n",
    "        # print(image_path)\n",
    "        img_ = Image.open(image_path)\n",
    "        image_name = image_path.split('/')[-1]\n",
    "\n",
    "        img_tensor = self.transforms(img_)\n",
    "        # img_.show()\n",
    "        img_label = image_name.split('_')[0]\n",
    "\n",
    "        img_label = tls.text2vec(img_label)\n",
    "        img_label = img_label.view(1, -1)[0]\n",
    "\n",
    "        return img_tensor, img_label\n",
    "\n",
    "\n",
    "\n",
    "        # return super().__getitem__(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return  self.list_image_path.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_original():\n",
    "    train_path = r\"./dataset/train/\"\n",
    "    test_path = r\"./dataset/test/\"\n",
    "\n",
    "    train_dataset = My_datasets(train_path)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataset = My_datasets(test_path)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    loss_fn=nn.MultiLabelSoftMarginLoss().to(args.device)\n",
    "    model = myResNet().to(args.device) \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "    # w=SummaryWriter(\"logs\")\n",
    "    total_step=0\n",
    "    save_step = 100\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        min_loss = 100000\n",
    "        for i,(imgs,targets) in enumerate(train_dataloader):\n",
    "            imgs=imgs.to(args.device)\n",
    "            targets=targets.to(args.device)\n",
    "            # print(imgs.shape)\n",
    "            # print(targets.shape)\n",
    "            outputs=model(imgs)\n",
    "            # print(outputs.shape)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                total_step+=1\n",
    "                if total_step % save_step == 0:\n",
    "                    save_step = max(save_step - 20 ,10)\n",
    "                    print(\"save model {}\".format(total_step))\n",
    "                torch.save(model.state_dict(),\"afterResNetmodel.pth\")\n",
    "        \n",
    "        print(\"epoch{}, loss:{}\".format(epoch, min_loss.item()))\n",
    "    \n",
    "\n",
    "    #         if i % 10000 == 0:\n",
    "    #             total_step+=1\n",
    "    #             print(\"训练{}次,loss:{}\".format((total_step-1)*10000, loss.item()))\n",
    "    # torch.save(model.state_dict(),\"afterResNetmodel.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val():\n",
    "    train_path = r\"./dataset/train/\"\n",
    "    test_path = r\"./dataset/test/\"\n",
    "\n",
    "    train_dataset = My_datasets(train_path)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataset = My_datasets(test_path)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    loss_fn=nn.MultiLabelSoftMarginLoss().to(args.device)\n",
    "    model = myResNet().to(args.device) \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "    # w=SummaryWriter(\"logs\")\n",
    "    total_step=0\n",
    "    save_step = 100\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        min_loss = 100000\n",
    "        for i,(imgs,targets) in enumerate(train_dataloader):\n",
    "            imgs=imgs.to(args.device)\n",
    "            targets=targets.to(args.device)\n",
    "            # print(imgs.shape)\n",
    "            # print(targets.shape)\n",
    "            outputs=model(imgs)\n",
    "            # print(outputs.shape)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                total_step+=1\n",
    "                if total_step % save_step == 0:\n",
    "                    save_step = max(save_step - 20 ,10)\n",
    "                    print(\"save model {}\".format(total_step))\n",
    "                torch.save(model.state_dict(),\"afterResNetmodel.pth\")\n",
    "        \n",
    "        print(\"epoch{}, loss:{}\".format(epoch, min_loss.item()))\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     model.eval()\n",
    "        #     test_data = My_datasets(\"./dataset/test/\")\n",
    "\n",
    "        #     test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "        #     test_length = test_data.__len__()\n",
    "        #     correct = 0;\n",
    "        #     for i, (imgs, lables) in enumerate(test_dataloader):\n",
    "        #         imgs = imgs.to(args.device)\n",
    "        #         lables = lables.to(args.device)\n",
    "\n",
    "        #         lables = lables.view(-1, args.captcha_array.__len__())\n",
    "\n",
    "        #         lables_text = tls.vec2text(lables)\n",
    "        #         predict_outputs = model(imgs)\n",
    "        #         predict_outputs = predict_outputs.view(-1, args.captcha_array.__len__())\n",
    "        #         predict_labels = tls.vec2text(predict_outputs)\n",
    "        #         if predict_labels == lables_text:\n",
    "        #             correct += 1\n",
    "        #             # print(\"预测正确：正确值:{},预测值:{}\".format(lables_text, predict_labels))\n",
    "        #         else:\n",
    "        #             print(\"预测失败:正确值:{},预测值:{}\".format(lables_text, predict_labels))\n",
    "        #         # m(imgs)\n",
    "            \n",
    "        #     cor_rate = correct / test_length * 100\n",
    "        #     # if cor_rate\n",
    "        #     print(\"正确率{}\".format(cor_rate))\n",
    "    \n",
    "\n",
    "    #         if i % 10000 == 0:\n",
    "    #             total_step+=1\n",
    "    #             print(\"训练{}次,loss:{}\".format((total_step-1)*10000, loss.item()))\n",
    "    # torch.save(model.state_dict(),\"afterResNetmodel.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 1\n",
      "save model 2\n",
      "save model 3\n",
      "save model 4\n",
      "save model 5\n",
      "save model 6\n",
      "save model 7\n",
      "save model 8\n",
      "save model 9\n",
      "save model 10\n",
      "save model 11\n",
      "save model 12\n",
      "save model 13\n",
      "save model 14\n",
      "save model 15\n",
      "save model 16\n",
      "save model 17\n",
      "save model 18\n",
      "save model 19\n",
      "save model 20\n",
      "save model 21\n",
      "save model 22\n",
      "save model 23\n",
      "epoch0, loss:0.12704098224639893\n",
      "save model 24\n",
      "save model 25\n",
      "save model 26\n",
      "save model 27\n",
      "epoch1, loss:0.12682418525218964\n",
      "save model 28\n",
      "save model 29\n",
      "save model 30\n",
      "save model 31\n",
      "save model 32\n",
      "save model 33\n",
      "save model 34\n",
      "epoch2, loss:0.12642735242843628\n",
      "save model 35\n",
      "save model 36\n",
      "save model 37\n",
      "save model 38\n",
      "save model 39\n",
      "save model 40\n",
      "save model 41\n",
      "save model 42\n",
      "save model 43\n",
      "save model 44\n",
      "epoch3, loss:0.12514029443264008\n",
      "save model 45\n",
      "save model 46\n",
      "save model 47\n",
      "save model 48\n",
      "save model 49\n",
      "save model 50\n",
      "save model 51\n",
      "save model 52\n",
      "save model 53\n",
      "save model 54\n",
      "save model 55\n",
      "save model 56\n",
      "save model 57\n",
      "save model 58\n",
      "save model 59\n",
      "save model 60\n",
      "save model 61\n",
      "save model 62\n",
      "save model 63\n",
      "epoch4, loss:0.1200409010052681\n",
      "save model 64\n",
      "save model 65\n",
      "save model 66\n",
      "save model 67\n",
      "save model 68\n",
      "save model 69\n",
      "save model 70\n",
      "save model 71\n",
      "save model 72\n",
      "save model 73\n",
      "save model 74\n",
      "save model 75\n",
      "save model 76\n",
      "save model 77\n",
      "save model 78\n",
      "save model 79\n",
      "save model 80\n",
      "save model 81\n",
      "save model 82\n",
      "save model 83\n",
      "save model 84\n",
      "save model 85\n",
      "save model 86\n",
      "save model 87\n",
      "save model 88\n",
      "save model 89\n",
      "save model 90\n",
      "save model 91\n",
      "epoch5, loss:0.10480333864688873\n",
      "save model 92\n",
      "save model 93\n",
      "save model 94\n",
      "save model 95\n",
      "save model 96\n",
      "save model 97\n",
      "save model 98\n",
      "save model 99\n",
      "save model 100\n",
      "save model 101\n",
      "save model 102\n",
      "save model 103\n",
      "save model 104\n",
      "save model 105\n",
      "save model 106\n",
      "save model 107\n",
      "save model 108\n",
      "save model 109\n",
      "save model 110\n",
      "save model 111\n",
      "save model 112\n",
      "save model 113\n",
      "save model 114\n",
      "save model 115\n",
      "save model 116\n",
      "save model 117\n",
      "save model 118\n",
      "save model 119\n",
      "epoch6, loss:0.08214064687490463\n",
      "save model 120\n",
      "save model 121\n",
      "save model 122\n",
      "save model 123\n",
      "save model 124\n",
      "save model 125\n",
      "save model 126\n",
      "save model 127\n",
      "save model 128\n",
      "save model 129\n",
      "save model 130\n",
      "save model 131\n",
      "save model 132\n",
      "save model 133\n",
      "save model 134\n",
      "save model 135\n",
      "save model 136\n",
      "save model 137\n",
      "save model 138\n",
      "save model 139\n",
      "save model 140\n",
      "save model 141\n",
      "save model 142\n",
      "save model 143\n",
      "save model 144\n",
      "save model 145\n",
      "save model 146\n",
      "save model 147\n",
      "save model 148\n",
      "save model 149\n",
      "save model 150\n",
      "epoch7, loss:0.04741649329662323\n",
      "save model 151\n",
      "save model 152\n",
      "save model 153\n",
      "save model 154\n",
      "save model 155\n",
      "save model 156\n",
      "save model 157\n",
      "save model 158\n",
      "save model 159\n",
      "save model 160\n",
      "save model 161\n",
      "save model 162\n",
      "save model 163\n",
      "save model 164\n",
      "save model 165\n",
      "save model 166\n",
      "save model 167\n",
      "save model 168\n",
      "save model 169\n",
      "save model 170\n",
      "save model 171\n",
      "save model 172\n",
      "save model 173\n",
      "save model 174\n",
      "save model 175\n",
      "save model 176\n",
      "save model 177\n",
      "save model 178\n",
      "save model 179\n",
      "save model 180\n",
      "save model 181\n",
      "save model 182\n",
      "save model 183\n",
      "save model 184\n",
      "epoch8, loss:0.01917639747262001\n",
      "save model 185\n",
      "save model 186\n",
      "save model 187\n",
      "save model 188\n",
      "save model 189\n",
      "save model 190\n",
      "save model 191\n",
      "save model 192\n",
      "save model 193\n",
      "save model 194\n",
      "save model 195\n",
      "save model 196\n",
      "save model 197\n",
      "save model 198\n",
      "save model 199\n",
      "save model 200\n",
      "save model 201\n",
      "save model 202\n",
      "save model 203\n",
      "save model 204\n",
      "save model 205\n",
      "save model 206\n",
      "save model 207\n",
      "save model 208\n",
      "save model 209\n",
      "save model 210\n",
      "save model 211\n",
      "save model 212\n",
      "save model 213\n",
      "save model 214\n",
      "save model 215\n",
      "epoch9, loss:0.007355947047472\n",
      "save model 216\n",
      "save model 217\n",
      "save model 218\n",
      "save model 219\n",
      "save model 220\n",
      "save model 221\n",
      "save model 222\n",
      "save model 223\n",
      "save model 224\n",
      "save model 225\n",
      "save model 226\n",
      "save model 227\n",
      "save model 228\n",
      "save model 229\n",
      "save model 230\n",
      "epoch10, loss:0.0035527122672647238\n",
      "save model 231\n",
      "save model 232\n",
      "save model 233\n",
      "save model 234\n",
      "save model 235\n",
      "save model 236\n",
      "save model 237\n",
      "save model 238\n",
      "save model 239\n",
      "epoch11, loss:0.0019670389592647552\n",
      "save model 240\n",
      "save model 241\n",
      "save model 242\n",
      "save model 243\n",
      "save model 244\n",
      "save model 245\n",
      "save model 246\n",
      "epoch12, loss:0.0015585791552439332\n",
      "save model 247\n",
      "save model 248\n",
      "save model 249\n",
      "save model 250\n",
      "save model 251\n",
      "save model 252\n",
      "save model 253\n",
      "save model 254\n",
      "epoch13, loss:0.0009405374876223505\n",
      "save model 255\n",
      "save model 256\n",
      "save model 257\n",
      "epoch14, loss:0.0008694221032783389\n",
      "save model 258\n",
      "save model 259\n",
      "save model 260\n",
      "save model 261\n",
      "save model 262\n",
      "save model 263\n",
      "epoch15, loss:0.0007519739447161555\n",
      "save model 264\n",
      "save model 265\n",
      "save model 266\n",
      "save model 267\n",
      "save model 268\n",
      "epoch16, loss:0.0007939577335491776\n",
      "save model 269\n",
      "save model 270\n",
      "save model 271\n",
      "save model 272\n",
      "epoch17, loss:0.0006123697385191917\n",
      "save model 273\n",
      "save model 274\n",
      "save model 275\n",
      "save model 276\n",
      "save model 277\n",
      "save model 278\n",
      "save model 279\n",
      "save model 280\n",
      "epoch18, loss:0.0004417959717102349\n",
      "save model 281\n",
      "save model 282\n",
      "epoch19, loss:0.000360411184374243\n"
     ]
    }
   ],
   "source": [
    "train_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_path = r\"./dataset/train/\"\n",
    "    test_path = r\"./dataset/test/\"\n",
    "\n",
    "    train_dataset = My_datasets(train_path)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataset = My_datasets(test_path)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    model = myResNet().to(args.device) \n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  \n",
    "\n",
    "    train_epochs_loss = []\n",
    "    valid_epochs_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_epoch_loss = []\n",
    "        acc, nums = 0, train_dataset.__len__()\n",
    "        # =========================train=======================\n",
    "        # test_length = test_data.__len__()\n",
    "        for idx, (inputs, label) in enumerate(tqdm(train_dataloader)):\n",
    "            inputs = inputs.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label = label.view(-1, args.captcha_array.__len__())\n",
    "            label_text = tls.vec2text(label)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            outputs = outputs.view(-1, args.captcha_array.__len__())\n",
    "            predict_labels = tls.vec2text(outputs)\n",
    "            if predict_labels == label_text:\n",
    "                acc += 1\n",
    "            #     print(\"预测正确：正确值:{},预测值:{}\".format(label_text, predict_labels))\n",
    "            # else:\n",
    "            #     print(\"预测失败:正确值:{},预测值:{}\".format(label_text, predict_labels))\n",
    "            \n",
    "            loss = criterion(outputs, label.long()).to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0) #用来梯度裁剪\n",
    "            optimizer.step()\n",
    "            train_epoch_loss.append(loss.item())\n",
    "\n",
    "            # outputs = predict_outputs.view(-1, args.captcha_array.__len__())\n",
    "            # predict_labels = tls.vectotext(predict_outputs)\n",
    "\n",
    "            # acc += (outputs == label).sum()\n",
    "            # nums += label.size()[0]\n",
    "            # print(nums)\n",
    "\n",
    "        train_epochs_loss.append(np.average(train_epoch_loss))\n",
    "        \n",
    "        # print(type(train_epochs_loss))\n",
    "        # print(np.average(train_epoch_loss))\n",
    "\n",
    "        acc = float(acc)\n",
    "\n",
    "        train_acc.append(100 * acc / nums)\n",
    "        # print(100 * acc)\n",
    "        print(\"train acc = {:.3f}%, loss = {}\".format(100 * acc / nums, np.average(train_epoch_loss)))\n",
    "        # =========================val=========================\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_epoch_loss = []\n",
    "            acc, nums = 0, val_dataloader.__len__()\n",
    "\n",
    "            for idx, (inputs, label) in enumerate(tqdm(val_dataloader)):\n",
    "                inputs = inputs.to(args.device)  # .to(torch.float)\n",
    "                label = label.to(args.device)\n",
    "                # outputs = model(inputs)\n",
    "\n",
    "                label = label.view(-1, args.captcha_array.__len__())\n",
    "                label_text = tls.vec2text(label)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                outputs = outputs.view(-1, args.captcha_array.__len__())\n",
    "                predict_labels = tls.vec2text(outputs)\n",
    "                if predict_labels == label_text:\n",
    "                    acc += 1\n",
    "                #     print(\"预测正确：正确值:{},预测值:{}\".format(label_text, predict_labels))\n",
    "                # else:\n",
    "                #     print(\"预测失败:正确值:{},预测值:{}\".format(label_text, predict_labels))\n",
    "\n",
    "                loss = criterion(outputs, label)\n",
    "                val_epoch_loss.append(loss.item())\n",
    "\n",
    "                # acc += (outputs == label).sum()\n",
    "                # nums += label.size()[0]\n",
    "                # nums += label.size()[0]\n",
    "\n",
    "\n",
    "            valid_epochs_loss.append(np.average(val_epoch_loss))\n",
    "            val_acc.append(100 * acc / nums)\n",
    "\n",
    "            print(\"epoch = {}, valid acc = {:.2f}%, loss = {}\".format(epoch, 100 * acc / nums, np.average(val_epoch_loss)))\n",
    "    # torch.save(model,\"model.pth\")\n",
    "    torch.save(model.state_dict(),\"af_bestmodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pred():\n",
    "    m = myResNet()\n",
    "    # m.load_state_dict(torch.load(\"best_model.pth\",map_location = args.device))\n",
    "    m.load_state_dict(torch.load(\"afterResNetmodel.pth\",map_location = args.device), strict=False)\n",
    "\n",
    "    m.to(args.device)\n",
    "    m.eval()\n",
    "    test_data = My_datasets(\"./dataset/test/\")\n",
    "\n",
    "    test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "    test_length = test_data.__len__()\n",
    "    correct = 0;\n",
    "    for i, (imgs, lables) in enumerate(test_dataloader):\n",
    "        imgs = imgs.to(args.device)\n",
    "        lables = lables.to(args.device)\n",
    "\n",
    "        lables = lables.view(-1, args.captcha_array.__len__())\n",
    "\n",
    "        lables_text = tls.vec2text(lables)\n",
    "        predict_outputs = m(imgs)\n",
    "        predict_outputs = predict_outputs.view(-1, args.captcha_array.__len__())\n",
    "        predict_labels = tls.vec2text(predict_outputs)\n",
    "        if predict_labels == lables_text:\n",
    "            correct += 1\n",
    "            # print(\"预测正确：正确值:{},预测值:{}\".format(lables_text, predict_labels))\n",
    "        else:\n",
    "            print(\"预测失败:正确值:{},预测值:{}\".format(lables_text, predict_labels))\n",
    "        # m(imgs)\n",
    "    print(\"正确率{}\".format(correct / test_length * 100))\n",
    "def pred_pic(pic_path):\n",
    "    img=Image.open(pic_path)\n",
    "    tersor_img=transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((60,160)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img=tersor_img(img).to(args.device)\n",
    "    print(img.shape)\n",
    "    img=torch.reshape(img,(-1,1,60,160))\n",
    "    print(img.shape)\n",
    "    m = torch.load(\"model.pth\").to(args.device)\n",
    "    outputs = m(img)\n",
    "    outputs=outputs.view(-1,len(args.captcha_array))\n",
    "    outputs_lable=tls.vec2text(outputs)\n",
    "    print(outputs_lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测失败:正确值:jbgl,预测值:ibgl\n",
      "预测失败:正确值:w164,预测值:wz64\n",
      "预测失败:正确值:g80u,预测值:880u\n",
      "预测失败:正确值:t9eo,预测值:t980\n",
      "预测失败:正确值:jln3,预测值:jln9\n",
      "预测失败:正确值:k0g5,预测值:kog5\n",
      "预测失败:正确值:etni,预测值:etnd\n",
      "预测失败:正确值:oqhx,预测值:oqhh\n",
      "预测失败:正确值:q045,预测值:qo45\n",
      "预测失败:正确值:pdhb,预测值:pdbb\n",
      "预测失败:正确值:17ig,预测值:12ig\n",
      "预测失败:正确值:sz4c,预测值:5z4c\n",
      "正确率94.0\n"
     ]
    }
   ],
   "source": [
    "test_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_path = \"./dataset/train/\"\n",
    "    test_path = \"./dataset/test/\"\n",
    "\n",
    "    train_dataset = My_datasets(train_path)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataset = My_datasets(test_path)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    model = myResNet().to(args.device) \n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  \n",
    "\n",
    "    train_epochs_loss = []\n",
    "    valid_epochs_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_epoch_loss = []\n",
    "        acc, nums = 0, train_dataset.__len__()\n",
    "        # =========================train=======================\n",
    "        # test_length = test_data.__len__()\n",
    "        for idx, (inputs, label) in enumerate(tqdm(train_dataloader)):\n",
    "            inputs = inputs.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label = label.view(-1, args.captcha_array.__len__())\n",
    "            label_text = tls.vec2text(label)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            outputs = outputs.view(-1, args.captcha_array.__len__())\n",
    "            predict_labels = tls.vec2text(outputs)\n",
    "            if predict_labels == label_text:\n",
    "                acc += 1\n",
    "            #     print(\"预测正确：正确值:{},预测值:{}\".format(label_text, predict_labels))\n",
    "            # else:\n",
    "            #     print(\"预测失败:正确值:{},预测值:{}\".format(label_text, predict_labels))\n",
    "            \n",
    "            loss = criterion(outputs, label.long()).to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0) #用来梯度裁剪\n",
    "            optimizer.step()\n",
    "            train_epoch_loss.append(loss.item())\n",
    "\n",
    "            # outputs = predict_outputs.view(-1, args.captcha_array.__len__())\n",
    "            # predict_labels = tls.vec2text(predict_outputs)\n",
    "\n",
    "            # acc += (outputs == label).sum()\n",
    "            # nums += label.size()[0]\n",
    "            # print(nums)\n",
    "\n",
    "        train_epochs_loss.append(np.average(train_epoch_loss))\n",
    "        \n",
    "        # print(type(train_epochs_loss))\n",
    "        # print(np.average(train_epoch_loss))\n",
    "\n",
    "        acc = float(acc)\n",
    "\n",
    "        train_acc.append(100 * acc / nums)\n",
    "        # print(100 * acc)\n",
    "        print(\"train acc = {:.3f}%, loss = {}\".format(100 * acc / nums, np.average(train_epoch_loss)))\n",
    "        # =========================val=========================\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_epoch_loss = []\n",
    "            acc, nums = 0, val_dataloader.__len__()\n",
    "\n",
    "            for idx, (inputs, label) in enumerate(tqdm(val_dataloader)):\n",
    "                inputs = inputs.to(args.device)  # .to(torch.float)\n",
    "                label = label.to(args.device)\n",
    "                # outputs = model(inputs)\n",
    "\n",
    "                label = label.view(-1, args.captcha_array.__len__())\n",
    "                label_text = tls.vec2text(label)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                outputs = outputs.view(-1, args.captcha_array.__len__())\n",
    "                predict_labels = tls.vec2text(outputs)\n",
    "                if predict_labels == label_text:\n",
    "                    acc += 1\n",
    "                #     print(\"预测正确：正确值:{},预测值:{}\".format(label_text, predict_labels))\n",
    "                # else:\n",
    "                #     print(\"预测失败:正确值:{},预测值:{}\".format(label_text, predict_labels))\n",
    "\n",
    "                loss = criterion(outputs, label)\n",
    "                val_epoch_loss.append(loss.item())\n",
    "\n",
    "                # acc += (outputs == label).sum()\n",
    "                # nums += label.size()[0]\n",
    "                # nums += label.size()[0]\n",
    "\n",
    "\n",
    "            valid_epochs_loss.append(np.average(val_epoch_loss))\n",
    "            val_acc.append(100 * acc / nums)\n",
    "\n",
    "            print(\"epoch = {}, valid acc = {:.2f}%, loss = {}\".format(epoch, 100 * acc / nums, np.average(val_epoch_loss)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # =========================plot==========================\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "#     plt.subplot(121)\n",
    "#     plt.plot(train_epochs_loss[:])\n",
    "#     plt.title(\"train_loss\")\n",
    "#     plt.subplot(122)\n",
    "#     plt.plot(train_epochs_loss, '-o', label=\"train_loss\")\n",
    "#     plt.plot(valid_epochs_loss, '-o', label=\"valid_loss\")\n",
    "#     plt.title(\"epochs_loss\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "#     # =========================save model=====================\n",
    "#     torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "\n",
    "# def pred(val):\n",
    "#     model = Net(1, 32, 16, 2)\n",
    "#     model.load_state_dict(torch.load('model.pth'))\n",
    "#     model.eval()\n",
    "#     val = torch.tensor(val).reshape(1, -1).float()\n",
    "#     # 需要转换成相应的输入shape，而且得带上batch_size，因此转换成shape=(1,1)这样的形状\n",
    "#     res = model(val)\n",
    "#     # real: tensor([[-5.2095, -0.9326]], grad_fn=<AddmmBackward0>) 需要找到最大值所在的列数，就是标签\n",
    "#     res = res.max(axis=1)[1].item()\n",
    "#     print(\"predicted label is {}, {} {} 8\".format(res, val.item(), ('>' if res == 1 else '<')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
